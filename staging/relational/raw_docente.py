#!/usr/bin/env python3
"""
raw_docente.py

MÃ³dulo para processamento e consolidaÃ§Ã£o de dados brutos de docentes CAPES.

DescriÃ§Ã£o:
    Este script processa arquivos CSV de docentes da CAPES (formato br-capes-colsucup-docente-*.csv),
    consolida dados de mÃºltiplos anos, remove duplicatas e carrega os dados na tabela raw_docente
    do PostgreSQL para uso posterior em dimensÃµes do Data Warehouse.

Funcionalidades:
    - Leitura automÃ¡tica de mÃºltiplos arquivos CSV de docentes
    - ConsolidaÃ§Ã£o de dados de diferentes anos base (2021, 2022, 2023, etc.)
    - NormalizaÃ§Ã£o de nomes de colunas para padrÃ£o snake_case
    - Limpeza e transformaÃ§Ã£o de dados:
      * ConversÃ£o de tipos de dados apropriados
      * NormalizaÃ§Ã£o de campos texto (upper, strip)
      * Tratamento de valores nulos
      * AdiÃ§Ã£o de metadados (fonte_arquivo, created_at)
    - RemoÃ§Ã£o de duplicatas por (id_pessoa + ano_base)
    - Carga automÃ¡tica no PostgreSQL (padrÃ£o ativado)

Fonte de Dados:
    - Arquivos: br-capes-colsucup-docente-{ANO}-{DATA}.csv
    - LocalizaÃ§Ã£o: staging/data/
    - Formato: CSV com delimitador padrÃ£o
    - Origem: Plataforma Sucupira/CAPES

Estrutura da Tabela raw_docente:
    Campos de IdentificaÃ§Ã£o:
    - id_pessoa: Identificador Ãºnico do docente no sistema CAPES
    - nm_docente: Nome completo do docente
    - ano_base: Ano base dos dados (2021, 2022, 2023, etc.)
    
    Dados Institucionais:
    - cd_entidade_capes: CÃ³digo da instituiÃ§Ã£o no sistema CAPES
    - cd_entidade_emec: CÃ³digo da instituiÃ§Ã£o no sistema e-MEC
    - sg_entidade_ensino: Sigla da instituiÃ§Ã£o de ensino
    - nm_entidade_ensino: Nome completo da instituiÃ§Ã£o
    - ds_dependencia_administrativa: Tipo de dependÃªncia (PÃºblica/Privada)
    - cs_status_juridico: Status jurÃ­dico da instituiÃ§Ã£o
    - nm_municipio_programa_ies: MunicÃ­pio da instituiÃ§Ã£o
    - sg_uf_programa: Sigla da UF da instituiÃ§Ã£o
    
    Dados do Programa:
    - cd_programa_ies: CÃ³digo do programa de pÃ³s-graduaÃ§Ã£o
    - nm_programa_ies: Nome do programa
    - nm_grau_programa: Grau do programa (Mestrado/Doutorado/Mestrado Profissional)
    - nm_modalidade_programa: Modalidade do programa (AcadÃªmico/Profissional)
    - cd_conceito_programa: Conceito CAPES do programa (3-7)
    
    Dados de Ãrea de Conhecimento:
    - cd_area_avaliacao: CÃ³digo da Ã¡rea de avaliaÃ§Ã£o
    - nm_area_avaliacao: Nome da Ã¡rea de avaliaÃ§Ã£o
    - nm_grande_area_conhecimento: Grande Ã¡rea do conhecimento
    - nm_area_conhecimento: Ãrea de conhecimento especÃ­fica
    
    Dados do Docente:
    - ds_categoria_docente: Categoria (Permanente/Colaborador/Visitante)
    - ds_regime_trabalho: Regime de trabalho (Integral/Parcial/Horista)
    - ds_faixa_etaria: Faixa etÃ¡ria do docente
    - tp_sexo_docente: Sexo do docente (M/F)
    - in_doutor: Indicador se possui doutorado (SIM/NÃƒO)
    - an_titulacao: Ano de obtenÃ§Ã£o da titulaÃ§Ã£o mÃ¡xima
    - nm_grau_titulacao: Grau de titulaÃ§Ã£o (Doutorado/Mestrado/etc.)
    - nm_area_basica_titulacao: Ãrea bÃ¡sica da titulaÃ§Ã£o
    - sg_ies_titulacao: Sigla da IES tituladora
    - cd_cat_bolsa_produtividade: Categoria de bolsa de produtividade (se houver)
    - in_coordenador_ppg: Indicador se Ã© coordenador de PPG (SIM/NÃƒO)
    
    Campos TÃ©cnicos:
    - tp_documento_docente: Tipo do documento (RG/CPF/Passaporte)
    - nr_documento_docente: NÃºmero do documento
    - an_nascimento_docente: Ano de nascimento
    - ds_tipo_nacionalidade_docente: Tipo de nacionalidade
    - nm_pais_nacionalidade_docente: PaÃ­s de nacionalidade
    - ds_tipo_vinculo_docente_ies: Tipo de vÃ­nculo com a IES
    - nm_ies_titulacao: Nome completo da IES tituladora
    - nm_pais_ies_titulacao: PaÃ­s da IES tituladora
    
    Metadados:
    - fonte_arquivo: Nome do arquivo CSV de origem
    - created_at: Timestamp de processamento (data/hora da carga)

Processo de ETL:
    1. ExtraÃ§Ã£o:
       - Localiza todos os arquivos CSV de docentes no diretÃ³rio staging/data/
       - LÃª cada arquivo com pandas.read_csv()
       - Adiciona metadados de origem (fonte_arquivo)
    
    2. TransformaÃ§Ã£o:
       - Normaliza nomes de colunas (AN_BASE â†’ ano_base)
       - Consolida DataFrames de mÃºltiplos anos
       - Limpa dados (strip, upper case em campos texto)
       - Remove duplicatas mantendo apenas um registro por (id_pessoa, ano_base)
       - Adiciona timestamp de processamento (created_at)
       - Reordena colunas priorizando campos importantes
    
    3. Carga:
       - Salva automaticamente no PostgreSQL (tabela raw_docente)
       - Usa method='multi' e chunksize=1000 para otimizaÃ§Ã£o
       - Substitui tabela existente (if_exists='replace')

DeduplicaÃ§Ã£o:
    - CritÃ©rio: (id_pessoa, ano_base) Ãºnico
    - EstratÃ©gia: MantÃ©m primeira ocorrÃªncia de cada combinaÃ§Ã£o
    - EstatÃ­sticas reportadas no log de execuÃ§Ã£o

ValidaÃ§Ãµes:
    - Verifica existÃªncia de arquivos CSV no diretÃ³rio
    - Valida conexÃ£o com PostgreSQL antes de inserir
    - Reporta contadores de registros em cada etapa
    - Mostra estatÃ­sticas finais (docentes Ãºnicos, anos, instituiÃ§Ãµes, UFs)

ConfiguraÃ§Ã£o:
    VariÃ¡veis de Ambiente (arquivo .env):
    - DB_HOST ou POSTGRES_HOST: EndereÃ§o do servidor PostgreSQL
    - DB_PORT ou POSTGRES_PORT: Porta do PostgreSQL (padrÃ£o: 5432)
    - DB_NAME ou POSTGRES_DB: Nome do banco de dados
    - DB_USER ou POSTGRES_USER: UsuÃ¡rio do banco
    - DB_PASS ou POSTGRES_PASSWORD: Senha do banco

Uso:
    # Modo padrÃ£o (carrega no PostgreSQL automaticamente)
    python3 staging/relational/raw_docente.py
    
    # Processar sem carregar no PostgreSQL
    python3 staging/relational/raw_docente.py --no-postgres
    
    # Especificar nome de tabela customizado
    python3 staging/relational/raw_docente.py --table nome_tabela_custom

Argumentos:
    --postgres: Habilita carga no PostgreSQL (PADRÃƒO: ativado)
    --no-postgres: Desabilita carga no PostgreSQL (apenas processa em memÃ³ria)
    --table NOME: Nome da tabela destino no PostgreSQL (padrÃ£o: raw_docente)

DependÃªncias:
    - pandas: ManipulaÃ§Ã£o de dados
    - sqlalchemy: ConexÃ£o e operaÃ§Ãµes com PostgreSQL
    - python-dotenv: Gerenciamento de variÃ¡veis de ambiente
    - argparse: Processamento de argumentos de linha de comando

SaÃ­da:
    - Tabela raw_docente no PostgreSQL com dados consolidados
    - Logs detalhados no console com estatÃ­sticas de processamento
    - Amostra de 5 registros para verificaÃ§Ã£o

ObservaÃ§Ãµes:
    - Processamento otimizado com chunks de 1000 registros
    - Suporta mÃºltiplos arquivos CSV automaticamente
    - MantÃ©m histÃ³rico por ano_base (nÃ£o remove anos anteriores)
    - Dados sÃ£o substituÃ­dos completamente a cada execuÃ§Ã£o (replace)

Performance:
    - ~330K registros processados em segundos
    - DeduplicaÃ§Ã£o resulta em ~254K registros Ãºnicos
    - ~92K docentes Ãºnicos identificados

Autor: UFMS - Data Warehouse CAPES/OES/NPG
Data de CriaÃ§Ã£o: 2025
Ãšltima AtualizaÃ§Ã£o: 09/10/2025
"""

import argparse
import os
import glob
from pathlib import Path
from typing import Dict, List

import pandas as pd
from sqlalchemy import create_engine, text
from dotenv import load_dotenv

# Carregar variÃ¡veis de ambiente
load_dotenv()

DEFAULT_TABLE = "raw_docente"


def save_to_postgres(df: pd.DataFrame, table_name: str) -> bool:
    """Salva o DataFrame na tabela indicada do PostgreSQL."""
    # Tentar primeiro variÃ¡veis POSTGRES_*, depois DB_* como fallback
    host = os.getenv("POSTGRES_HOST") or os.getenv("DB_HOST")
    port = os.getenv("POSTGRES_PORT") or os.getenv("DB_PORT")
    database = os.getenv("POSTGRES_DB") or os.getenv("DB_NAME")
    username = os.getenv("POSTGRES_USER") or os.getenv("DB_USER")
    password = os.getenv("POSTGRES_PASSWORD") or os.getenv("DB_PASS")

    conn_string = f"postgresql://{username}:{password}@{host}:{port}/{database}"

    try:
        print(f"ðŸ”— Conectando ao PostgreSQL: {host}:{port}/{database}")
        engine = create_engine(conn_string)

        with engine.connect() as conn:
            row = conn.execute(text("SELECT version()")).fetchone()
            if row:
                print(f"âœ… Conectado (versÃ£o: {row[0][:50]}...)")
            else:
                print("âœ… Conectado ao PostgreSQL.")

        print(f"ðŸ’¾ Gravando tabela {table_name}...")
        df.to_sql(
            table_name,
            engine,
            if_exists="replace",
            index=False,
            method="multi",
            chunksize=1000,
        )
        print("âœ… Dados enviados ao PostgreSQL.")
        return True

    except Exception as exc:  # pylint: disable=broad-except
        print(f"âŒ Erro ao conectar/salvar no PostgreSQL: {exc}")
        print("ðŸ’¡ Verifique se o banco estÃ¡ disponÃ­vel e as variÃ¡veis de ambiente foram definidas.")
        return False


def normalize_column_names(df: pd.DataFrame) -> pd.DataFrame:
    """Normaliza nomes das colunas para padrÃ£o snake_case."""
    rename_map: Dict[str, str] = {
        "AN_BASE": "ano_base",
        "CD_AREA_AVALIACAO": "cd_area_avaliacao",
        "NM_AREA_AVALIACAO": "nm_area_avaliacao",
        "NM_GRANDE_AREA_CONHECIMENTO": "nm_grande_area_conhecimento",
        "NM_AREA_CONHECIMENTO": "nm_area_conhecimento",
        "CD_PROGRAMA_IES": "cd_programa_ies",
        "NM_PROGRAMA_IES": "nm_programa_ies",
        "NM_GRAU_PROGRAMA": "nm_grau_programa",
        "NM_MODALIDADE_PROGRAMA": "nm_modalidade_programa",
        "CD_CONCEITO_PROGRAMA": "cd_conceito_programa",
        "CD_ENTIDADE_CAPES": "cd_entidade_capes",
        "CD_ENTIDADE_EMEC": "cd_entidade_emec",
        "SG_ENTIDADE_ENSINO": "sg_entidade_ensino",
        "NM_ENTIDADE_ENSINO": "nm_entidade_ensino",
        "DS_DEPENDENCIA_ADMINISTRATIVA": "ds_dependencia_administrativa",
        "CS_STATUS_JURIDICO": "cs_status_juridico",
        "NM_MUNICIPIO_PROGRAMA_IES": "nm_municipio_programa_ies",
        "SG_UF_PROGRAMA": "sg_uf_programa",
        "NM_REGIAO": "nm_regiao",
        "ID_PESSOA": "id_pessoa",
        "TP_DOCUMENTO_DOCENTE": "tp_documento_docente",
        "NR_DOCUMENTO_DOCENTE": "nr_documento_docente",
        "NM_DOCENTE": "nm_docente",
        "AN_NASCIMENTO_DOCENTE": "an_nascimento_docente",
        "DS_FAIXA_ETARIA": "ds_faixa_etaria",
        "DS_TIPO_NACIONALIDADE_DOCENTE": "ds_tipo_nacionalidade_docente",
        "NM_PAIS_NACIONALIDADE_DOCENTE": "nm_pais_nacionalidade_docente",
        "DS_CATEGORIA_DOCENTE": "ds_categoria_docente",
        "DS_TIPO_VINCULO_DOCENTE_IES": "ds_tipo_vinculo_docente_ies",
        "DS_REGIME_TRABALHO": "ds_regime_trabalho",
        "CD_CAT_BOLSA_PRODUTIVIDADE": "cd_cat_bolsa_produtividade",
        "IN_DOUTOR": "in_doutor",
        "AN_TITULACAO": "an_titulacao",
        "NM_GRAU_TITULACAO": "nm_grau_titulacao",
        "CD_AREA_BASICA_TITULACAO": "cd_area_basica_titulacao",
        "NM_AREA_BASICA_TITULACAO": "nm_area_basica_titulacao",
        "SG_IES_TITULACAO": "sg_ies_titulacao",
        "NM_IES_TITULACAO": "nm_ies_titulacao",
        "NM_PAIS_IES_TITULACAO": "nm_pais_ies_titulacao",
        "ID_ADD_FOTO_PROGRAMA": "id_add_foto_programa",
        "ID_ADD_FOTO_PROGRAMA_IES": "id_add_foto_programa_ies",
    }
    
    return df.rename(columns=rename_map)


def load_and_consolidate_docente_files(data_dir: Path) -> pd.DataFrame:
    """Carrega e consolida todos os arquivos de docentes."""
    # Busca todos os arquivos de docentes
    pattern = str(data_dir / "br-capes-colsucup-docente-*.csv")
    docente_files = glob.glob(pattern)
    
    if not docente_files:
        raise FileNotFoundError(f"Nenhum arquivo de docente encontrado em {data_dir}")
    
    print(f"ðŸ“ Encontrados {len(docente_files)} arquivos de docentes:")
    for file_path in sorted(docente_files):
        print(f"   â€¢ {Path(file_path).name}")
    
    dataframes: List[pd.DataFrame] = []
    total_records = 0
    
    for file_path in sorted(docente_files):
        file_name = Path(file_path).name
        print(f"ðŸ“¥ Processando {file_name}...")
        
        try:
            # LÃª o arquivo com encoding latin-1 e separador ;
            # Tenta primeiro com engine padrÃ£o, depois com python se falhar
            try:
                df = pd.read_csv(file_path, encoding="latin-1", sep=";", dtype=str)
            except Exception:
                print(f"   âš ï¸  Tentando com engine='python'...")
                df = pd.read_csv(file_path, encoding="latin-1", sep=";", dtype=str, engine="python")
            
            # Normaliza nomes das colunas
            df = normalize_column_names(df)
            
            # Adiciona metadados
            df["fonte_arquivo"] = file_name
            df["created_at"] = pd.Timestamp.now().normalize()
            
            print(f"   âœ” {len(df):,} registros carregados")
            total_records += len(df)
            dataframes.append(df)
            
        except Exception as exc:
            print(f"   âŒ Erro ao processar {file_name}: {exc}")
            continue
    
    if not dataframes:
        raise ValueError("Nenhum arquivo foi processado com sucesso")
    
    print(f"\nðŸ”„ Consolidando {total_records:,} registros de {len(dataframes)} arquivos...")
    df_consolidated = pd.concat(dataframes, ignore_index=True)
    
    return df_consolidated


def clean_and_deduplicate(df: pd.DataFrame) -> pd.DataFrame:
    """Limpa os dados e remove duplicatas."""
    print("ðŸ§¹ Limpando dados...")
    
    # Limpeza bÃ¡sica dos textos
    for col in df.columns:
        if df[col].dtype == object and col not in ["fonte_arquivo", "created_at"]:
            df[col] = df[col].fillna("").astype(str).str.strip()
    
    # ConversÃµes especÃ­ficas
    numeric_cols = ["ano_base", "id_pessoa", "an_nascimento_docente", "an_titulacao", 
                   "cd_area_avaliacao", "cd_programa_ies", "cd_conceito_programa",
                   "cd_entidade_capes", "cd_entidade_emec", "cd_area_basica_titulacao",
                   "id_add_foto_programa", "id_add_foto_programa_ies"]
    
    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0).astype(int)
    
    # Normaliza campos de texto importantes
    text_normalize_cols = ["sg_entidade_ensino", "sg_uf_programa", "tp_documento_docente",
                          "ds_categoria_docente", "in_doutor", "sg_ies_titulacao"]
    
    for col in text_normalize_cols:
        if col in df.columns:
            df[col] = df[col].str.upper()
    
    print(f"   âœ” Dados limpos: {len(df):,} registros")
    
    # Remove duplicatas baseado em ID_PESSOA + ANO_BASE + CD_PROGRAMA_IES
    print("ðŸ” Removendo duplicatas...")
    duplicates_before = len(df)
    
    # Identifica duplicatas baseado nas chaves principais
    dedup_keys = ["id_pessoa", "ano_base", "cd_programa_ies"]
    df_dedup = df.drop_duplicates(subset=dedup_keys, keep="last")
    
    duplicates_removed = duplicates_before - len(df_dedup)
    print(f"   âœ” {duplicates_removed:,} duplicatas removidas")
    print(f"   âœ” {len(df_dedup):,} registros Ãºnicos mantidos")
    
    return df_dedup


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Gera a tabela raw_docente consolidando todos os arquivos de docentes CAPES."
    )
    parser.add_argument(
        "--postgres",
        action="store_true",
        default=True,
        help="Envia a tabela tambÃ©m para o PostgreSQL (default: SIM, use --no-postgres para desabilitar).",
    )
    parser.add_argument(
        "--no-postgres",
        action="store_false",
        dest="postgres",
        help="Desabilita o envio para PostgreSQL.",
    )
    parser.add_argument(
        "--table",
        default=DEFAULT_TABLE,
        help=f"Nome da tabela destino no PostgreSQL (default: {DEFAULT_TABLE}).",
    )
    args = parser.parse_args()

    base_dir = Path(__file__).resolve().parent
    data_dir = (base_dir / ".." / "data").resolve()

    print("ðŸŽ“ === PROCESSAMENTO RAW_DOCENTE CAPES ===")
    print(f"ðŸ“– DiretÃ³rio de dados: {data_dir}")
    
    # Carrega e consolida todos os arquivos
    df = load_and_consolidate_docente_files(data_dir)
    
    # Limpa e remove duplicatas
    df_clean = clean_and_deduplicate(df)
    
    # Reorganiza colunas para melhor visualizaÃ§Ã£o
    priority_cols = [
        "id_pessoa", "nm_docente", "ano_base", "nm_entidade_ensino", "sg_uf_programa",
        "ds_categoria_docente", "nm_programa_ies", "nm_area_avaliacao", "in_doutor",
        "fonte_arquivo", "created_at"
    ]
    other_cols = [col for col in df_clean.columns if col not in priority_cols]
    final_cols = priority_cols + other_cols
    df_final = df_clean[[col for col in final_cols if col in df_clean.columns]]
    
    # Salva no PostgreSQL (padrÃ£o ativado)
    if args.postgres:
        save_to_postgres(df_final, args.table)
    else:
        print("ðŸ’¡ PostgreSQL desabilitado (--no-postgres).")
        print("ðŸ’¡ Dados processados apenas em memÃ³ria (sem geraÃ§Ã£o de arquivos).")

    # EstatÃ­sticas finais
    print("\nðŸ“Š EstatÃ­sticas finais:")
    print(f"   â€¢ Total de registros: {len(df_final):,}")
    print(f"   â€¢ Docentes Ãºnicos: {df_final['id_pessoa'].nunique():,}")
    print(f"   â€¢ Anos de base: {sorted(df_final['ano_base'].unique())}")
    print(f"   â€¢ InstituiÃ§Ãµes: {df_final['nm_entidade_ensino'].nunique():,}")
    print(f"   â€¢ UFs: {sorted(df_final['sg_uf_programa'].unique())}")
    
    print("\nðŸ“‹ Amostra dos dados:")
    display_cols = ["id_pessoa", "nm_docente", "ano_base", "nm_entidade_ensino", "sg_uf_programa"]
    print(df_final[display_cols].head().to_string())


if __name__ == "__main__":
    main()
