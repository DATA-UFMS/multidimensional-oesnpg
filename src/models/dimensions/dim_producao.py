#!/usr/bin/env python3
"""
dim_producao.py

MÃ³dulo para criaÃ§Ã£o e gerenciamento da dimensÃ£o de produÃ§Ãµes no Data Warehouse.

DescriÃ§Ã£o:
    Este mÃ³dulo implementa o processo de ETL (Extract, Transform, Load) para a dimensÃ£o
    de produÃ§Ãµes acadÃªmicas da pÃ³s-graduaÃ§Ã£o brasileira. ContÃ©m informaÃ§Ãµes sobre as
    diferentes produÃ§Ãµes (artigos, livros, capÃ­tulos, trabalhos em eventos, etc.) 
    registradas nos programas de pÃ³s-graduaÃ§Ã£o.
    
    A dimensÃ£o inclui:
    - Dados bÃ¡sicos de identificaÃ§Ã£o (id_producao, tipo, subtipo)
    - InformaÃ§Ãµes descritivas (tÃ­tulo, ano, editora, periÃ³dico)
    - ClassificaÃ§Ãµes (tipo de produÃ§Ã£o, subtipo, natureza)
    - MÃ©tricas de qualidade (issn, isbn, doi)

Fontes de Dados:
    - Base Principal: add_producao_*.parquet (MinIO S3) - mÃºltiplos anos
    - Fallback: Arquivo local em data/raw_producao/
    
Estrutura da DimensÃ£o:
    - producao_sk: Surrogate key (chave substituta sequencial, inicia em 0)
    - id_producao: ID da produÃ§Ã£o no sistema CAPES
    - tipo_producao: Tipo de produÃ§Ã£o (bibliogrÃ¡fica, tÃ©cnica, artÃ­stica)
    - subtipo_producao: Subtipo especÃ­fico
    - titulo_producao: TÃ­tulo da produÃ§Ã£o
    - ano_producao: Ano de publicaÃ§Ã£o/realizaÃ§Ã£o
    - nome_periodico: Nome do periÃ³dico (para artigos)
    - issn: ISSN do periÃ³dico
    - isbn: ISBN (para livros)
    - doi: DOI da produÃ§Ã£o
    - editora: Editora
    - pais_publicacao: PaÃ­s de publicaÃ§Ã£o
    - idioma: Idioma da produÃ§Ã£o
    - natureza_producao: Natureza (completo, resumo, etc.)
    - meio_divulgacao: Meio de divulgaÃ§Ã£o
    - ano_base: Ano base de referÃªncia

Registro SK=0:
    - Registro especial com producao_sk=0 representa valores desconhecidos
    - Usado para integridade referencial nas tabelas fato

Processo ETL:
    1. ExtraÃ§Ã£o: Carrega dados do MinIO (S3) de mÃºltiplos anos ou arquivo local
    2. TransformaÃ§Ã£o: 
       - PadronizaÃ§Ã£o de nomes de colunas
       - Tratamento de valores nulos
       - ConversÃ£o de tipos de dados
       - Truncamento de campos VARCHAR
       - RemoÃ§Ã£o de duplicatas
    3. Carga: InserÃ§Ã£o em chunks de 500 registros no PostgreSQL

Regras de NegÃ³cio:
    - Registro SK=0 sempre incluÃ­do para valores desconhecidos
    - Duplicatas removidas mantendo registro mais recente (ano_base DESC)
    - Campos VARCHAR truncados para respeitar limites da tabela
    - Valores nulos tratados com valores padrÃ£o ('NÃƒO INFORMADO', 'DESCONHECIDO')

Autor: Sistema DW OESNPG
Data: Outubro/2025
VersÃ£o: 1.0
"""

import os
import sys
import pandas as pd
import numpy as np
import logging
import time
from datetime import datetime
from typing import Optional, Dict, Any, List

# Adicionar o diretÃ³rio raiz ao path
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))
sys.path.insert(0, project_root)

from dotenv import load_dotenv
from src.core.core import get_db_manager

# Carregar variÃ¡veis de ambiente
load_dotenv()

# Configurar logging
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] [%(levelname)s] %(message)s')
logger = logging.getLogger(__name__)

def get_logger():
    return logger


def carregar_dados_producao():
    """
    Carrega dados de produÃ§Ã£o do MinIO S3 ou arquivo local.
    
    Tenta carregar de mÃºltiplos arquivos add_producao_*.parquet do MinIO.
    Se falhar, tenta carregar de arquivo local.
    
    Returns:
        pd.DataFrame: DataFrame com dados de produÃ§Ã£o ou None se falhar
        
    Raises:
        Exception: Se nÃ£o conseguir carregar de nenhuma fonte
    """
    logger.info("ğŸš€ Iniciando carregamento de dados de produÃ§Ã£o...")
    
    # Tentar carregar do MinIO primeiro
    try:
        import s3fs
        
        endpoint = os.getenv('MINIO_ENDPOINT')
        bucket = os.getenv('MINIO_BUCKET')
        access_key = os.getenv('MINIO_ACCESS_KEY')
        secret_key = os.getenv('MINIO_SECRET_KEY')
        
        logger.info(f"Conectando ao MinIO: {endpoint}")
        
        # Garantir que endpoint nÃ£o tenha http:// duplicado
        if endpoint and not endpoint.startswith('http'):
            endpoint_url = f'http://{endpoint}'
        else:
            endpoint_url = endpoint or 'http://localhost:9000'
            
        s3 = s3fs.S3FileSystem(
            key=access_key,
            secret=secret_key,
            client_kwargs={'endpoint_url': endpoint_url}
        )
        
        # Listar arquivos de produÃ§Ã£o disponÃ­veis (apenas add_producao, nÃ£o _autor)
        files = s3.glob(f'{bucket}/add_capes/*')
        producao_files = [str(f) for f in files if 'add_producao_' in str(f) and '_autor' not in str(f) and str(f).endswith('.parquet') and ('2023' in str(f) or '2024' in str(f))]
        
        if not producao_files:
            logger.warning("Nenhum arquivo add_producao 2023-2024 encontrado no MinIO")
            raise FileNotFoundError("Arquivos de produÃ§Ã£o 2023-2024 nÃ£o encontrados no MinIO")
        
        logger.info(f"âœ… Encontrados {len(producao_files)} arquivos de produÃ§Ã£o no MinIO")
        
        # Carregar e concatenar todos os arquivos
        dfs = []
        for file_path in sorted(producao_files):
            logger.info(f"ğŸ“¥ Carregando: {file_path}")
            df_temp = pd.read_parquet(f's3://{file_path}', storage_options={'key': access_key, 'secret': secret_key, 'client_kwargs': {'endpoint_url': endpoint_url}})
            # Extrair ano do nome do arquivo
            year = str(file_path).split('_')[-1].replace('.parquet', '')
            df_temp['ano_base'] = year
            dfs.append(df_temp)
            logger.info(f"   âœ“ {len(df_temp):,} registros carregados")
        
        df = pd.concat(dfs, ignore_index=True)
        logger.info(f"âœ… Total de {len(df):,} produÃ§Ãµes carregadas do MinIO")
        
        return df
        
    except ImportError:
        logger.warning("âš ï¸ Biblioteca s3fs nÃ£o instalada. Tentando arquivo local...")
    except Exception as e:
        logger.warning(f"âš ï¸ Erro ao carregar do MinIO: {e}")
        logger.info("Tentando carregar de arquivo local...")
    
    # Fallback: tentar carregar arquivo local
    try:
        local_paths = [
            'data/raw_producao/add_producao.parquet',
            'staging/data/add_producao.parquet',
            '../data/add_producao.parquet'
        ]
        
        for local_path in local_paths:
            full_path = os.path.join(project_root, local_path)
            if os.path.exists(full_path):
                logger.info(f"ğŸ“‚ Carregando arquivo local: {full_path}")
                df = pd.read_parquet(full_path)
                logger.info(f"âœ… {len(df):,} produÃ§Ãµes carregadas localmente")
                return df
        
        raise FileNotFoundError("Nenhum arquivo de produÃ§Ã£o encontrado localmente")
        
    except Exception as e:
        logger.error(f"âŒ Erro ao carregar dados de produÃ§Ã£o: {e}")
        raise


def transformar_dados_producao(df: pd.DataFrame) -> pd.DataFrame:
    """
    Transforma e padroniza dados de produÃ§Ã£o.
    
    Args:
        df: DataFrame bruto de produÃ§Ã£o
        
    Returns:
        pd.DataFrame: DataFrame transformado e limpo
    """
    logger.info("ğŸ”„ Transformando dados de produÃ§Ã£o...")
    
    df_transformado = df.copy()
    
    # Renomear colunas para padronizaÃ§Ã£o
    colunas_map = {
        'ID_ADD_PRODUCAO': 'id_producao',
        'ID_TIPO_PRODUCAO': 'id_tipo_producao',
        'ID_SUBTIPO_PRODUCAO': 'id_subtipo_producao',
        'NM_TIPO_PRODUCAO': 'tipo_producao',
        'NM_SUBTIPO_PRODUCAO': 'subtipo_producao',
        'DS_TITULO': 'titulo_producao',
        'AN_BASE': 'ano_producao',
        'NM_PERIODICO': 'nome_periodico',
        'DS_ISSN': 'issn',
        'DS_ISBN': 'isbn',
        'DS_DOI': 'doi',
        'NM_EDITORA': 'editora',
        'SG_PAIS_PUBLICACAO': 'pais_publicacao',
        'NM_IDIOMA': 'idioma',
        'DS_NATUREZA': 'natureza_producao',
        'DS_MEIO_DIVULGACAO': 'meio_divulgacao',
        'AN_BASE_PRODUCAO': 'ano_base'
    }
    
    # Renomear colunas que existem
    cols_to_rename = {k: v for k, v in colunas_map.items() if k in df_transformado.columns}
    df_transformado = df_transformado.rename(columns=cols_to_rename)
    
    # Garantir que colunas essenciais existam
    colunas_essenciais = [
        'id_producao', 'tipo_producao', 'subtipo_producao', 'titulo_producao',
        'ano_producao', 'ano_base'
    ]
    
    for col in colunas_essenciais:
        if col not in df_transformado.columns:
            # Tentar encontrar coluna similar
            similar_cols = [c for c in df_transformado.columns if col.replace('_', '').lower() in c.replace('_', '').lower()]
            if similar_cols:
                df_transformado[col] = df_transformado[similar_cols[0]]
            else:
                df_transformado[col] = 'NÃƒO INFORMADO' if col in ['tipo_producao', 'subtipo_producao', 'titulo_producao'] else 0
    
    # Tratar valores nulos
    df_transformado['tipo_producao'] = df_transformado['tipo_producao'].fillna('NÃƒO INFORMADO')
    df_transformado['subtipo_producao'] = df_transformado['subtipo_producao'].fillna('NÃƒO INFORMADO')
    df_transformado['titulo_producao'] = df_transformado['titulo_producao'].fillna('SEM TÃTULO')
    df_transformado['ano_producao'] = pd.to_numeric(df_transformado['ano_producao'], errors='coerce').fillna(0).astype(int)
    
    # Truncar campos VARCHAR para limites da tabela
    if 'titulo_producao' in df_transformado.columns:
        df_transformado['titulo_producao'] = df_transformado['titulo_producao'].astype(str).str[:500]
    
    if 'nome_periodico' in df_transformado.columns:
        df_transformado['nome_periodico'] = df_transformado['nome_periodico'].fillna('NÃƒO INFORMADO').astype(str).str[:300]
    
    if 'editora' in df_transformado.columns:
        df_transformado['editora'] = df_transformado['editora'].fillna('NÃƒO INFORMADO').astype(str).str[:200]
    
    if 'issn' in df_transformado.columns:
        df_transformado['issn'] = df_transformado['issn'].fillna('').astype(str).str[:20]
    
    if 'isbn' in df_transformado.columns:
        df_transformado['isbn'] = df_transformado['isbn'].fillna('').astype(str).str[:20]
    
    if 'doi' in df_transformado.columns:
        df_transformado['doi'] = df_transformado['doi'].fillna('').astype(str).str[:100]
    
    if 'pais_publicacao' in df_transformado.columns:
        df_transformado['pais_publicacao'] = df_transformado['pais_publicacao'].fillna('BRA').astype(str).str[:10]
    
    if 'idioma' in df_transformado.columns:
        df_transformado['idioma'] = df_transformado['idioma'].fillna('PORTUGUÃŠS').astype(str).str[:50]
    
    if 'natureza_producao' in df_transformado.columns:
        df_transformado['natureza_producao'] = df_transformado['natureza_producao'].fillna('NÃƒO INFORMADO').astype(str).str[:100]
    
    if 'meio_divulgacao' in df_transformado.columns:
        df_transformado['meio_divulgacao'] = df_transformado['meio_divulgacao'].fillna('NÃƒO INFORMADO').astype(str).str[:100]
    
    # Selecionar apenas colunas finais
    colunas_finais = [
        'id_producao', 'tipo_producao', 'subtipo_producao', 'titulo_producao',
        'ano_producao', 'ano_base'
    ]
    
    # Adicionar colunas opcionais se existirem
    colunas_opcionais = [
        'nome_periodico', 'issn', 'isbn', 'doi', 'editora', 
        'pais_publicacao', 'idioma', 'natureza_producao', 'meio_divulgacao'
    ]
    
    for col in colunas_opcionais:
        if col in df_transformado.columns:
            colunas_finais.append(col)
    
    df_transformado = df_transformado[colunas_finais]
    
    # Remover duplicatas mantendo o registro mais recente
    logger.info(f"Registros antes de remover duplicatas: {len(df_transformado):,}")
    df_transformado = df_transformado.sort_values('ano_base', ascending=False)
    df_transformado = df_transformado.drop_duplicates(subset=['id_producao'], keep='first')
    logger.info(f"Registros apÃ³s remover duplicatas: {len(df_transformado):,}")
    
    logger.info(f"âœ… TransformaÃ§Ã£o concluÃ­da: {len(df_transformado):,} produÃ§Ãµes")
    
    return df_transformado


def criar_tabela(db):
    """
    Cria a tabela dim_producao no banco de dados.
    
    Args:
        db: InstÃ¢ncia do DatabaseManager
    """
    logger.info("ğŸ”¨ Criando tabela dim_producao...")
    
    sql_drop = "DROP TABLE IF EXISTS dim_producao CASCADE;"
    
    sql_create = """
    CREATE TABLE dim_producao (
        producao_sk SERIAL PRIMARY KEY,
        id_producao VARCHAR(50) UNIQUE NOT NULL,
        tipo_producao VARCHAR(100) NOT NULL,
        subtipo_producao VARCHAR(100),
        titulo_producao VARCHAR(500),
        ano_producao INTEGER,
        nome_periodico VARCHAR(300),
        issn VARCHAR(20),
        isbn VARCHAR(20),
        doi VARCHAR(100),
        editora VARCHAR(200),
        pais_publicacao VARCHAR(10),
        idioma VARCHAR(50),
        natureza_producao VARCHAR(100),
        meio_divulgacao VARCHAR(100),
        ano_base VARCHAR(4) NOT NULL,
        data_carga TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        CONSTRAINT chk_ano_producao CHECK (ano_producao = 0 OR (ano_producao >= 1900 AND ano_producao <= 2100))
    );
    
    CREATE INDEX idx_dim_producao_id ON dim_producao(id_producao);
    CREATE INDEX idx_dim_producao_tipo ON dim_producao(tipo_producao);
    CREATE INDEX idx_dim_producao_ano ON dim_producao(ano_producao);
    CREATE INDEX idx_dim_producao_ano_base ON dim_producao(ano_base);
    """
    
    # Usar engine.begin() para executar comandos DDL
    with db.engine.begin() as conn:
        conn.exec_driver_sql(sql_drop)
        conn.exec_driver_sql(sql_create)
        
        # Inserir registro SK=0 para valores desconhecidos/nÃ£o informados
        sql_insert_sk0 = """
        INSERT INTO dim_producao (
            producao_sk, id_producao, tipo_producao, subtipo_producao, 
            titulo_producao, ano_producao, ano_base
        ) VALUES (
            0, '0', 'NÃ£o informado', 'NÃ£o informado', 
            'NÃ£o informado', 0, '0'
        );
        
        -- Resetar sequence para comeÃ§ar do 1
        SELECT setval('dim_producao_producao_sk_seq', 1, false);
        """
        conn.exec_driver_sql(sql_insert_sk0)
    
    logger.info("âœ… Tabela dim_producao criada com sucesso!")


def inserir_dados_producao(df: pd.DataFrame, db):
    """
    Insere dados de produÃ§Ã£o na tabela dim_producao em chunks.
    
    Args:
        df: DataFrame com dados transformados
        db: InstÃ¢ncia do DatabaseManager
    """
    logger.info(f"ğŸ“¥ Inserindo {len(df):,} produÃ§Ãµes na tabela dim_producao...")
    
    chunk_size = 500
    total_chunks = (len(df) + chunk_size - 1) // chunk_size
    
    logger.info(f"Dividindo em {total_chunks} chunks de {chunk_size} registros")
    
    start_time = time.time()
    
    for i in range(0, len(df), chunk_size):
        chunk_num = (i // chunk_size) + 1
        chunk = df.iloc[i:i + chunk_size].copy()
        
        logger.info(f"ğŸ“¦ Inserindo chunk {chunk_num}/{total_chunks} - Registros {i} a {min(i + chunk_size, len(df))} ({len(chunk)} registros)")
        
        chunk_start = time.time()
        
        try:
            inserir_chunk_direto(chunk, db)
            
            chunk_time = time.time() - chunk_start
            logger.info(f"âœ… Chunk {chunk_num} inserido com sucesso em {chunk_time:.2f}s")
            
        except Exception as e:
            logger.error(f"âŒ Erro ao inserir chunk {chunk_num}: {e}")
            raise
    
    elapsed_time = time.time() - start_time
    logger.info(f"âœ… InserÃ§Ã£o concluÃ­da em {elapsed_time:.2f}s ({len(df)/elapsed_time:.0f} registros/s)")
    
    # Verificar total inserido
    result = db.execute_query("SELECT COUNT(*) as total FROM dim_producao WHERE producao_sk > 0")
    total_inserido = result[0]['total'] if result else 0
    
    logger.info(f"âœ… InserÃ§Ã£o concluÃ­da! Total de registros inseridos: {total_inserido:,}")
    logger.info(f"ğŸ“Š Esperado: {len(df):,}, Inserido: {total_inserido:,}")


def inserir_chunk_direto(chunk, db):
    """
    Insere um chunk de dados diretamente usando to_sql do pandas.
    
    Args:
        chunk: DataFrame com chunk de dados
        db: Gerenciador de banco de dados
    """
    chunk.to_sql(
        'dim_producao',
        db.engine,
        if_exists='append',
        index=False,
        method='multi',
        chunksize=100
    )
    
    logger.info("âœ… Chunk inserido com sucesso usando to_sql")


def executar_etl_producao():
    """
    Executa o processo completo de ETL para dim_producao.
    """
    logger.info("=" * 80)
    logger.info("INICIANDO ETL - DIMENSÃƒO PRODUÃ‡ÃƒO")
    logger.info("=" * 80)
    
    start_time = time.time()
    
    try:
        # 1. Carregar dados
        df = carregar_dados_producao()
        
        if df is None or len(df) == 0:
            logger.error("âŒ Nenhum dado de produÃ§Ã£o carregado!")
            return
        
        # 2. Transformar dados
        df_transformado = transformar_dados_producao(df)
        
        # 3. Conectar ao banco
        logger.info("ğŸ”Œ Conectando ao banco de dados...")
        db = get_db_manager()
        
        # 4. Criar tabela
        criar_tabela(db)
        
        # 5. Inserir dados
        inserir_dados_producao(df_transformado, db)
        
        elapsed_time = time.time() - start_time
        logger.info("=" * 80)
        logger.info(f"ğŸ‰ ETL CONCLUÃDO COM SUCESSO em {elapsed_time:.2f}s!")
        logger.info("=" * 80)
        
        # EstatÃ­sticas finais
        result = db.execute_query("""
            SELECT 
                COUNT(*) as total,
                COUNT(DISTINCT tipo_producao) as tipos_distintos,
                COUNT(DISTINCT ano_producao) as anos_distintos,
                MIN(ano_producao) as ano_min,
                MAX(ano_producao) as ano_max
            FROM dim_producao 
            WHERE producao_sk > 0
        """)
        
        if len(result) > 0:
            stats = result[0]
            logger.info(f"ğŸ“Š Total de produÃ§Ãµes: {stats['total']:,}")
            logger.info(f"ğŸ“Š Tipos distintos: {stats['tipos_distintos']:,}")
            logger.info(f"ğŸ“Š Anos distintos: {stats['anos_distintos']:,}")
            logger.info(f"ğŸ“Š PerÃ­odo: {stats['ano_min']} a {stats['ano_max']}")
        
    except Exception as e:
        logger.error(f"âŒ Erro durante ETL: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    executar_etl_producao()
